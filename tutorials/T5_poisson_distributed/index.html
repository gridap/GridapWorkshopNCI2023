<!doctype html> <html lang=en > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <script src="/GridapWorkshopNCI2023/libs/lunr/lunr.min.js"></script> <script src="/GridapWorkshopNCI2023/libs/lunr/lunr_index.js"></script> <script src="/GridapWorkshopNCI2023/libs/lunr/lunrclient.min.js"></script> <link rel=stylesheet  href="/GridapWorkshopNCI2023/libs/katex/katex.min.css"> <link rel=stylesheet  href="/GridapWorkshopNCI2023/libs/highlight/github.min.css"> <link rel=stylesheet  href="/GridapWorkshopNCI2023/css/franklin.css"> <link rel=stylesheet  href="/GridapWorkshopNCI2023/css/poole_hyde.css"> <link rel=stylesheet  href="/GridapWorkshopNCI2023/css/custom.css"> <style> html {font-size: 17px;} .franklin-content {position: relative; padding-left: 8%; padding-right: 5%; line-height: 1.35em;} @media (min-width: 940px) { .franklin-content {width: 100%; margin-left: auto; margin-right: auto;} } @media (max-width: 768px) { .franklin-content {padding-left: 6%; padding-right: 6%;} } </style> <link rel=apple-touch-icon  sizes=152x152  href="/GridapWorkshopNCI2023/assets/favicon/apple-touch-icon.png"> <link rel=icon  type="image/png" sizes=32x32  href="/GridapWorkshopNCI2023/assets/favicon/favicon-32x32.png"> <link rel=icon  type="image/png" sizes=16x16  href="/GridapWorkshopNCI2023/assets/favicon/favicon-16x16.png"> <link rel=manifest  href="/GridapWorkshopNCI2023/assets/favicon/site.webmanifest"> <link rel=mask-icon  href="/GridapWorkshopNCI2023/assets/favicon/safari-pinned-tab.svg" color="#5bbad5"> <meta name=msapplication-TileColor  content="#da532c"> <meta name=theme-color  content="#ffffff"> <title>Parallel distributed-memory Poisson tutorial</title> <style> .content {max-width: 60rem} </style> <div class=sidebar > <div class="container sidebar-sticky"> <div class=sidebar-about > <img src="/GridapWorkshopNCI2023/assets/logos.png" style="width: 500px; height: auto; display: inline"> <div style="font-weight: margin-bottom: 0.5em"><a href="/GridapWorkshopNCI2023/">28-29th, November, 2023</a> <span style="opacity: 0.9;">| Australian National University, Canberra</span></div> <br> <h1><a href="/GridapWorkshopNCI2023/">Introduction to Gridap: Simulating PDEs using finite elements in Julia</a></h1> <div style="line-height:18px; font-size: 15px; opacity: 0.85">by &nbsp; <a href="https://research.monash.edu/en/persons/santiago-badia">Santiago Badia</a>, &nbsp; <a href="https://amartinhuertas.github.io/">Alberto F. Martin</a>, &nbsp; <a href="https://github.com/JordiManyer">Jordi Manyer</a> </div> </div> <br> <style> </style> <nav class=sidebar-nav  style="opacity: 0.9; margin-bottom: 1.2cm;"> <a class="sidebar-nav-item " href="/GridapWorkshopNCI2023/"><b>Welcome</b></a> <a class="sidebar-nav-item " href="/GridapWorkshopNCI2023/venue/"><b>Venue</b></a> <a class="sidebar-nav-item " href="/GridapWorkshopNCI2023/schedule"><b>Schedule</b></a> <a class="sidebar-nav-item " href="/GridapWorkshopNCI2023/software_install/">Software install instructions</a> <a class="sidebar-nav-item " href="/GridapWorkshopNCI2023/tutorials/">Tutorials & Exercises</a> <a class="sidebar-nav-item " href="/GridapWorkshopNCI2023/references/">References</a> <a class="sidebar-nav-item " href="/GridapWorkshopNCI2023/grants/">Registration and grants for research students</a> <a class="sidebar-nav-item " href="/GridapWorkshopNCI2023/acks/">Acknowledgements</a> <a class="sidebar-nav-item " href="/GridapWorkshopNCI2023/flyer/">Event Flyer</a> <br> </nav> <form id=lunrSearchForm  name=lunrSearchForm > <input class=search-input  name=q  placeholder="Enter search term" type=text > <input type=submit  value=Search  formaction="/GridapWorkshopNCI2023/search/index.html"> </form> <br> <br> </div> </div> <div class="content container"> <div class=franklin-content > <h1 id=parallel_distributed-memory_poisson_tutorial ><a href="#parallel_distributed-memory_poisson_tutorial" class=header-anchor >Parallel distributed-memory Poisson tutorial</a></h1> <p>In this tutorial, we will use the MPI-emulated environment provided by <code>DebugArray</code> to interactively have a look at some key aspects of how <code>GridapDistributed</code> works.</p> <h2 id=setup ><a href="#setup" class=header-anchor >Setup</a></h2> <p>We will be using three packages:</p> <ul> <li><p><code>Gridap</code>, which provides the local FE framework</p> <li><p><code>PartitionedArrays</code>, which provides a generic library for MPI-distributed linear algebra</p> <li><p><code>GridapDistributed</code>, which provides the distributed layer on top of <code>Gridap</code></p> </ul> <pre><code class="julia hljs"><span class=hljs-keyword >using</span> Gridap
<span class=hljs-keyword >using</span> GridapDistributed
<span class=hljs-keyword >using</span> PartitionedArrays</code></pre> <p>We start by creating our distributed rank indices, which plays the role of the more traditional communicator &#40;such as MPI.COMM_WORLD&#41;.</p> <pre><code class="julia hljs">nprocs = (<span class=hljs-number >2</span>,<span class=hljs-number >1</span>)
ranks  = with_debug() <span class=hljs-keyword >do</span> distribute
  distribute(<span class=hljs-built_in >LinearIndices</span>((prod(nprocs),)))
<span class=hljs-keyword >end</span></code></pre> <h2 id=distributed_meshes_and_spaces ><a href="#distributed_meshes_and_spaces" class=header-anchor >Distributed meshes and spaces</a></h2> <p>We can create a distributed Cartesian model by passing the newly-created ranks to the serial constructor. Thanks to Julia&#39;s multiple-dispatch, this is pretty much the only change we will have to do to convert a serial Gridap code into a GridapDistributed code.</p> <pre><code class="julia hljs">domain = (<span class=hljs-number >0</span>,<span class=hljs-number >1</span>,<span class=hljs-number >0</span>,<span class=hljs-number >1</span>)
ncells = (<span class=hljs-number >4</span>,<span class=hljs-number >2</span>)
serial_model = CartesianDiscreteModel(domain,ncells)

model = CartesianDiscreteModel(ranks,nprocs,domain,ncells)</code></pre> <p>The created <code>DistributedDiscreteModel</code> is just a wrapper around a distributed array of serial models. One can get access to the local models with the method <code>local_views</code>, which is defined for most distributed structures in <code>GridapDistributed</code>.</p> <pre><code class="julia hljs">local_models = local_views(model)
display(local_models)</code></pre> <p>A key aspect in parallel programming is the concept of owned &amp; ghost ids. If we compare the number of cells of the distributed model to the number of cells of the local model portions, we can observe that the local models overlap:</p> <pre><code class="julia hljs">global_ncells = num_cells(model)
local_ncells  = map(num_cells,local_views(model))</code></pre> <p>This overlap is due to ghost cells. The Owned/Ghost layout of the current distributed model can be seen in the next figure:</p> <p><img src="/GridapWorkshopNCI2023/assets/literate_figures/distributed/gids_cells.png" alt="" /></p> <p>This information is also stored on the distributed model, and can be accessed in the following way:</p> <pre><code class="julia hljs">cell_gids = get_cell_gids(model)
local_cell_to_global_cell = map(local_to_global,partition(cell_gids))
local_cell_to_owner       = map(local_to_owner,partition(cell_gids))
owned_cell_to_local_cell  = map(own_to_local,partition(cell_gids))
ghost_cell_to_local_cell  = map(ghost_to_local,partition(cell_gids))</code></pre> <p>For each processor, we have</p> <ul> <li><p><code>local_to_global</code> - map from local cell ids to global cell ids</p> <li><p><code>local_to_owner</code> - map from local cell ids to the processor id of the cell owner</p> <li><p><code>own_to_local</code> - list of owned local cells</p> <li><p><code>ghost_to_local</code> - list of ghost local cells</p> </ul> <p>More documentation on this can be found in PartitionedArrays.jl</p> <p>We can then continue by creating a <code>DistributedFESpace</code>, which &#40;like <code>DistributedDiscreteModel</code>&#41; is a structure that contains:</p> <ul> <li><p>A distributed array of serial &#40;overlapped&#41; <code>FESpace</code>s</p> <li><p>The Owned/Ghost layout for the DoFs</p> </ul> <p>In this specific example here, and just for clarity, we do not impose any Dirichlet boundary conditions. Since the Poisson problem is defined up to a constant, and therefore is not uniquely defined without any Dirichlet boundary conditions, the problem will not be well-posed. However, this is not a problem for the purpose of this tutorial.</p> <pre><code class="julia hljs">feorder = <span class=hljs-number >1</span>
reffe = ReferenceFE(lagrangian,<span class=hljs-built_in >Float64</span>,feorder)
V = FESpace(model,reffe)
U = TrialFESpace(V)</code></pre> <p>The DoF layout can be seen in the following figure</p> <p><img src="/GridapWorkshopNCI2023/assets/literate_figures/distributed/gids_dofs.png" alt="" /></p> <p>and can be accessed as follows:</p> <pre><code class="julia hljs">dof_gids = V.gids
local_dofs_to_global_dof = map(local_to_global,partition(dof_gids))
local_dofs_to_owner      = map(local_to_owner,partition(dof_gids))
owned_dofs_to_local_dof  = map(own_to_local,partition(dof_gids))
ghost_dofs_to_local_dof  = map(ghost_to_local,partition(dof_gids))</code></pre> <h2 id=solving_the_linear_system ><a href="#solving_the_linear_system" class=header-anchor >Solving the linear system</a></h2> <p>We can now define the weak form and integrate as usual:</p> <pre><code class="julia hljs">degree = <span class=hljs-number >2</span>*order
Ω = Triangulation(model)
dΩ = Measure(Ω,degree)

f(x)   = cos(x[<span class=hljs-number >1</span>])
a(u,v) = ∫( ∇(v)⋅∇(u) )*dΩ
l(v)   = ∫( v*f )*dΩ

op = AffineFEOperator(a,l,V,U)
uh = solve(op)</code></pre> <p>Note that by default <code>solve</code> will use the Julia LU direct solver. In parallel, we provide a toy implementation that gathers the whole matrix into a single processor and solves the linear system there. This is not scalable, and should only be used for debugging/testing purposes.</p> <p>The design of scalable solvers is a very complex issue. Direct factorization solvers such as MUMPS or PARDISO can scale to a few hundred processors, but iterative preconditioned solvers are the only viable option for larger problems.</p> <p>As we will see in the next tutorial, Gridap provides interfaces to some of the most popular distributed linear algebra libraries, such as PETSc, through satellite packages. We are also in the process of providing Julia-native iterative solvers.</p> <h2 id=distributed_linear_algebra ><a href="#distributed_linear_algebra" class=header-anchor >Distributed linear algebra</a></h2> <p>Let&#39;s take a moment to have a look at the distributed linear system: As usual, one can access the system matrix and rhs as</p> <pre><code class="julia hljs">A  = get_matrix(op)
b  = get_vector(op)</code></pre> <p>We observe <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal">A</span></span></span></span> and <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>b</mi></mrow><annotation encoding="application/x-tex">b</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal">b</span></span></span></span> are now of type <code>PSparseMatrix</code> and <code>PVector</code>, respectively. These represent distributed equivalents of the serial <code>SparseMatrix</code> and <code>Vector</code> types. Each object holds their local part of the array &#40;both owned and ghost indices&#41; and information on the Owned/Ghost layout for their rows &#40;and columns&#41;.</p> <p>The local sub-matrices and sub-vectors can be accessed as follows:</p> <pre><code class="julia hljs">local_mats = partition(A)
owned_mats = own_values(A)

local_vectors = partition(b)
owned_vectors = own_values(b)</code></pre> <p>Here, <code>local_mats</code> and <code>local_vectors</code> contain the sub-matrices and sub-vectors for all the local DoFs &#40;owned and ghost, with overlapping between processors&#41;, while <code>owned_mats</code> and <code>owned_vectors</code> contain the sub-matrices and sub-vectors for the owned DoFs only &#40;no overlapping&#41;.</p> <p>The row/column layout of the <code>PSparseMatrix</code> can be accessed as follows:</p> <pre><code class="julia hljs">rows = axes(A,<span class=hljs-number >1</span>)
cols = axes(A,<span class=hljs-number >2</span>)</code></pre> <p>If we compare them to the DoF layout from the original space <span class=katex ><span class=katex-mathml ><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi></mrow><annotation encoding="application/x-tex">V</annotation></semantics></math></span><span class=katex-html  aria-hidden=true ><span class=base ><span class=strut  style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span></span></span></span>, we can see a couple major differences:</p> <pre><code class="julia hljs">owned_dofs_to_local_dof  = map(own_to_local,partition(dof_gids))
ghost_dofs_to_local_dof  = map(ghost_to_local,partition(dof_gids))

owned_rows_to_local_row  = map(own_to_local,partition(rows))
ghost_rows_to_local_row  = map(ghost_to_local,partition(rows))</code></pre> <p>First, the owned DoFs are not necessarily the first ones in the global ordering. However, owned rows are always the first ones in the global ordering. This reordering is done to comply with the standards set by other distributed linear algebra libraries, such as PETSc.</p> <p>Second, the number of ghosts in the dof layout is higher than the number of ghosts in the row layout. This is because the row layout only contains the ghosts indices that are needed to compute the local matrix-vector product.</p> <p>What we take away from this is that we cannot use a <code>PVector</code> of DoFs to solve the linear system and viceversa &#40;which is what we generally do in serial&#41;. Moreover, the ghost layout can also be different for the rows and columns. If we ever do this, we will get an error message:</p> <pre><code class="julia hljs">x = get_free_dof_values(uh) <span class=hljs-comment ># DoF layout</span>
A * x <span class=hljs-comment ># Error!</span>
A * b <span class=hljs-comment ># Error!</span></code></pre> <p>To allocate <code>PVectors</code> with a specific ghost layout, we can use the function <code>pfill</code>:</p> <pre><code class="julia hljs">x_r = pfill(<span class=hljs-number >0.0</span>,partition(axes(A,<span class=hljs-number >1</span>))) <span class=hljs-comment ># Row layout</span>
x_c = pfill(<span class=hljs-number >0.0</span>,partition(axes(A,<span class=hljs-number >2</span>))) <span class=hljs-comment ># Col layout</span>
x_r .= A * x_c <span class=hljs-comment ># OK</span></code></pre> <p>Despite this, we can use column and row <code>PVectors</code> to create <code>FEFunctions</code>. The index mapping will be taken care of by <code>GridapDistributed</code>, like so:</p> <pre><code class="julia hljs">vh = FEFunction(V,x_c)</code></pre>

<div class=page-foot >
  <div class=copyright >
    <a href="https://github.com/gridap/GridapWorkshopNCI2023"><b>Edit this page on <img class=github-logo  src="https://unpkg.com/ionicons@5.1.2/dist/svg/logo-github.svg"></b></a><br>
    Last modified: December 01, 2023. Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a> and the <a href="https://julialang.org">Julia programming language</a>.
  </div>
</div>
</div>
    </div>